{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364fc66b-1fa2-4bfa-9afa-8faf714e963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba212677-9d50-4eea-9134-f1ebcd6d6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class PropertyScraper:\n",
    "    def __init__(self, url, timeout=5):\n",
    "        self.url = url\n",
    "        self.data = []\n",
    "        self.driver = self._initialize_driver()\n",
    "        self.wait = WebDriverWait(self.driver, timeout=timeout)\n",
    "\n",
    "    def _initialize_driver(self):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--disable-http2\")\n",
    "        chrome_options.add_argument(\"--incognito\")\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "        chrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "        chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "        chrome_options.add_argument(\n",
    "            \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"\n",
    "        )\n",
    "\n",
    "        service = Service(\"C:\\\\webdrivers\\\\chromedriver.exe\")\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        driver.maximize_window()\n",
    "        return driver\n",
    "\n",
    "    def _wait_for_page_to_load(self):\n",
    "        title = self.driver.title\n",
    "        try:\n",
    "            self.wait.until(\n",
    "                lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "            )\n",
    "        except:\n",
    "            print(f'the webpage \"{title}\" did not get fully loaded')\n",
    "        else:\n",
    "            print(f'the webpage \"{title}\" did get fully loaded')\n",
    "\n",
    "    def access_website(self):\n",
    "        self.driver.get(self.url)\n",
    "        self._wait_for_page_to_load()\n",
    "\n",
    "    def search_properties(self, text):\n",
    "        try:\n",
    "            search_bar = self.wait.until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@id=\"keyword2\"]'))\n",
    "            )\n",
    "        except:\n",
    "            print(\"Timeout while locating Search Bar.\\n\")\n",
    "        else:\n",
    "            search_bar.send_keys(text)\n",
    "            time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            valid_option = self.wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"0\"]'))\n",
    "            )\n",
    "        except:\n",
    "            print(\"Timeout while locating valid search option.\\n\")\n",
    "        else:\n",
    "            valid_option.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            search_button = self.wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"searchform_search_btn\"]'))\n",
    "            )\n",
    "        except:\n",
    "            print(\"Timeout while clicking on \\\"Search\\\" button.\\n\")\n",
    "        else:\n",
    "            search_button.click()\n",
    "            self._wait_for_page_to_load()\n",
    "\n",
    "    def adjust_budget_slider(self, offset):\n",
    "        try:\n",
    "            slider = self.wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "            )\n",
    "        except:\n",
    "            print(\"Timeout while clicking on Budget slider circle.\\n\")\n",
    "        else:\n",
    "            actions = ActionChains(self.driver)\n",
    "            (\n",
    "                actions\n",
    "                .click_and_hold(slider)\n",
    "                .move_by_offset(offset, 0)\n",
    "                .release()\n",
    "                .perform()\n",
    "            )\n",
    "            time.sleep(2)\n",
    "\n",
    "    def apply_filters(self):\n",
    "        # Verified\n",
    "        verified = self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[3]/span[1]'))\n",
    "        )\n",
    "        verified.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Ready to Move\n",
    "        ready_to_move = self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[5]/span[1]'))\n",
    "        )\n",
    "        ready_to_move.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Scroll to right to uncover remaining filters\n",
    "        while True:\n",
    "            try:\n",
    "                filter_right_button = self.wait.until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//i[contains(@class,'iconS_Common_24 icon_upArrow cc__rightArrow')]\"))\n",
    "                )\n",
    "            except:\n",
    "                print(\"Timeout because we have uncovered all filters.\\n\")\n",
    "                break\n",
    "            else:\n",
    "                filter_right_button.click()\n",
    "                time.sleep(1)\n",
    "\n",
    "        # With Photos\n",
    "        with_photos = self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[6]/span[2]'))\n",
    "        )\n",
    "        with_photos.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # With Videos\n",
    "        with_videos = self.wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[7]/span[2]'))\n",
    "        )\n",
    "        with_videos.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "    def _extract_data(self, row, by, value):\n",
    "        try:\n",
    "            return row.find_element(by, value).text\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def scrape_webpage(self):\n",
    "        rows = self.driver.find_elements(By.CLASS_NAME, \"tupleNew__TupleContent\")\n",
    "        for row in rows:\n",
    "            property = {\n",
    "                \"name\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__headingNrera\"),\n",
    "                \"location\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__propType\"),\n",
    "                \"price\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__priceValWrap\")\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                elements = row.find_elements(By.CLASS_NAME, \"tupleNew__area1Type\")\n",
    "            except:\n",
    "                property[\"area\"], property[\"bhk\"] = [np.nan, np.nan]\n",
    "            else:\n",
    "                property[\"area\"], property[\"bhk\"] = [ele.text for ele in elements]\n",
    "\n",
    "            self.data.append(property)\n",
    "\n",
    "    def navigate_pages_and_scrape_data(self):\n",
    "        page_count = 0\n",
    "        while True:\n",
    "            page_count += 1\n",
    "            try:\n",
    "                self.scrape_webpage()\n",
    "                next_page_button = self.driver.find_element(By.XPATH, \"//a[normalize-space()='Next Page >']\")\n",
    "            except:\n",
    "                print(f\"We have scraped {page_count} pages.\\n\")\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    self.driver.execute_script(\"window.scrollBy(0, arguments[0].getBoundingClientRect().top - 100);\", next_page_button)\n",
    "                    time.sleep(2)\n",
    "                    self.wait.until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, \"//a[normalize-space()='Next Page >']\"))\n",
    "                    ).click()\n",
    "                    time.sleep(5)\n",
    "                except:\n",
    "                    print(\"Timeout while clicking on \\\"Next Page\\\".\\n\")\n",
    "\n",
    "    def clean_data_and_save_as_excel(self, file_name):\n",
    "        df_properties = (\n",
    "            pd\n",
    "            .DataFrame(self.data)\n",
    "            .drop_duplicates()\n",
    "            .apply(lambda col: col.str.strip().str.lower() if col.dtype == \"object\" else col)\n",
    "            .assign(\n",
    "                is_starred=lambda df_: df_.name.str.contains(\"\\n\").astype(int),\n",
    "                name=lambda df_: (\n",
    "                    df_\n",
    "                    .name\n",
    "                    .str.replace(\"\\n[0-9.]+\", \"\", regex=True)\n",
    "                    .str.strip()\n",
    "                    .replace(\"adroit district s\", \"adroit district's\")\n",
    "                ),\n",
    "                location=lambda df_: (\n",
    "                    df_\n",
    "                    .location\n",
    "                    .str.replace(\"chennai\", \"\")\n",
    "                    .str.strip()\n",
    "                    .str.replace(\",$\", \"\", regex=True)\n",
    "                    .str.split(\"in\")\n",
    "                    .str[-1]\n",
    "                    .str.strip()\n",
    "                ),\n",
    "                price=lambda df_: (\n",
    "                    df_\n",
    "                    .price\n",
    "                    .str.replace(\"â‚¹\", \"\")\n",
    "                    .apply(lambda val: float(val.replace(\"lac\", \"\").strip()) if \"lac\" in val else float(val.replace(\"cr\", \"\").strip()) * 100)\n",
    "                ),\n",
    "                area=lambda df_: (\n",
    "                    df_\n",
    "                    .area\n",
    "                    .str.replace(\"sqft\", \"\")\n",
    "                    .str.strip()\n",
    "                    .str.replace(\",\", \"\")\n",
    "                    .pipe(lambda ser: pd.to_numeric(ser))\n",
    "                ),\n",
    "                bhk=lambda df_: (\n",
    "                    df_\n",
    "                    .bhk\n",
    "                    .str.replace(\"bhk\", \"\")\n",
    "                    .str.strip()\n",
    "                    .pipe(lambda ser: pd.to_numeric(ser))\n",
    "                )\n",
    "            )\n",
    "            .rename(columns={\n",
    "                \"price\": \"price_lakhs\",\n",
    "                \"area\": \"area_sqft\"\n",
    "            })\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        df_properties.to_excel(f\"{file_name}.xlsx\", index=False)\n",
    "\n",
    "    def run(self, text=\"Chennai\", offset=-100, file_name=\"properties\"):\n",
    "        try:\n",
    "            self.access_website()\n",
    "            self.search_properties(text)\n",
    "            self.adjust_budget_slider(offset)\n",
    "            self.apply_filters()\n",
    "            self.navigate_pages_and_scrape_data()\n",
    "            self.clean_data_and_save_as_excel(file_name)\n",
    "        finally:\n",
    "            time.sleep(2)\n",
    "            self.driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = PropertyScraper(url=\"https://www.99acres.com/\")\n",
    "    scraper.run(\n",
    "        text=\"chennai\",\n",
    "        offset=-73,\n",
    "        file_name=\"chennai-properties\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scaping",
   "language": "python",
   "name": "web_scaping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
